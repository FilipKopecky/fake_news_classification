{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800, 5)\n",
      "(5200, 4)\n"
     ]
    }
   ],
   "source": [
    "# retrieve the data\n",
    "path = r'.\\data\\\\'\n",
    "\n",
    "train_data = pd.read_csv(f'{path}train.csv', delimiter=\",\")\n",
    "test_data = pd.read_csv(f'{path}test.csv', delimiter=\",\")\n",
    "\n",
    "# get shapes of datasets\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20795</th>\n",
       "      <td>20795</td>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20796</th>\n",
       "      <td>20796</td>\n",
       "      <td>N.F.L. Playoffs: Schedule, Matchups and Odds -...</td>\n",
       "      <td>Benjamin Hoffman</td>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20797</th>\n",
       "      <td>20797</td>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>20798</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20799</th>\n",
       "      <td>20799</td>\n",
       "      <td>What Keeps the F-35 Alive</td>\n",
       "      <td>David Swanson</td>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2          2                  Why the Truth Might Get You Fired   \n",
       "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4          4  Iranian woman jailed for fictional unpublished...   \n",
       "...      ...                                                ...   \n",
       "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
       "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "20799  20799                          What Keeps the F-35 Alive   \n",
       "\n",
       "                                          author  \\\n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "...                                          ...   \n",
       "20795                              Jerome Hudson   \n",
       "20796                           Benjamin Hoffman   \n",
       "20797  Michael J. de la Merced and Rachel Abrams   \n",
       "20798                                Alex Ansary   \n",
       "20799                              David Swanson   \n",
       "\n",
       "                                                    text  label  \n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1      Ever get the feeling your life circles the rou...      0  \n",
       "2      Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1  \n",
       "...                                                  ...    ...  \n",
       "20795  Rapper T. I. unloaded on black celebrities who...      0  \n",
       "20796  When the Green Bay Packers lost to the Washing...      0  \n",
       "20797  The Macy’s of today grew from the union of sev...      0  \n",
       "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1  \n",
       "20799    David Swanson is an author, activist, journa...      1  \n",
       "\n",
       "[20800 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get general overview of training data\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      20800 non-null  int64 \n",
      " 1   title   20242 non-null  object\n",
      " 2   author  18843 non-null  object\n",
      " 3   text    20761 non-null  object\n",
      " 4   label   20800 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 812.6+ KB\n",
      "Column \"title\" has 558 null values.\n",
      "Column \"author\" has 1957 null values.\n",
      "Column \"text\" has 39 null values.\n"
     ]
    }
   ],
   "source": [
    "# show data types and check nulls\n",
    "train_data.info(verbose=True, show_counts=True)\n",
    "\n",
    "for feature in train_data:\n",
    "    null_count = train_data[feature].isna().sum()\n",
    "    if null_count > 0:\n",
    "        print(f'Column \"{feature}\" has {null_count} null values.')\n",
    "if train_data.isna().sum().any() == False:\n",
    "    print('No feature has any null values.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From domain knowledge about news industry, we know that reliable articles always have transparent author(s) and are structured so they have both title (headline) and text (content). I decided to inspect labels distribution to see if missing values carry important information value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of labels where title is null:\n",
      "1    1.0\n",
      "Name: label, dtype: float64\n",
      "Distribution of labels where text is null:\n",
      "1    1.0\n",
      "Name: label, dtype: float64\n",
      "Distribution of labels where author is null:\n",
      "1    0.986714\n",
      "0    0.013286\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# filter null rows\n",
    "null_title = train_data[train_data['title'].isnull()]\n",
    "null_author = train_data[train_data['author'].isnull()]\n",
    "null_text = train_data[train_data['text'].isnull()]\n",
    "\n",
    "# inspect distribution of labels in among null rows\n",
    "print(\"Distribution of labels where title is null:\")\n",
    "print(null_title['label'].value_counts(normalize=True))\n",
    "print(\"Distribution of labels where text is null:\")\n",
    "print(null_text['label'].value_counts(normalize=True))\n",
    "print(\"Distribution of labels where author is null:\")\n",
    "print(null_author['label'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that absence of these key data indicates unreliable news. To keep this information I will impute those missing values with respective indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with respective indicator\n",
    "train_data['title'].fillna('missing title', inplace=True)\n",
    "train_data['text'].fillna('missing text', inplace=True)\n",
    "train_data['author'].fillna('missing author', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[house, dem, aide, we, didn, even, see, comey,...</td>\n",
       "      <td>[darrell, lucus]</td>\n",
       "      <td>[house, dem, aide, we, didn, even, see, comey,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[flynn, hillary, clinton, big, woman, on, camp...</td>\n",
       "      <td>[daniel, flynn]</td>\n",
       "      <td>[ever, get, the, feeling, your, life, circles,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[why, the, truth, might, get, you, fired]</td>\n",
       "      <td>[consortiumnews, com]</td>\n",
       "      <td>[why, the, truth, might, get, you, fired, octo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[15, civilians, killed, in, single, us, airstr...</td>\n",
       "      <td>[jessica, purkiss]</td>\n",
       "      <td>[videos, 15, civilians, killed, in, single, us...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[iranian, woman, jailed, for, fictional, unpub...</td>\n",
       "      <td>[howard, portnoy]</td>\n",
       "      <td>[print, an, iranian, woman, has, been, sentenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  [house, dem, aide, we, didn, even, see, comey,...   \n",
       "1   1  [flynn, hillary, clinton, big, woman, on, camp...   \n",
       "2   2          [why, the, truth, might, get, you, fired]   \n",
       "3   3  [15, civilians, killed, in, single, us, airstr...   \n",
       "4   4  [iranian, woman, jailed, for, fictional, unpub...   \n",
       "\n",
       "                  author                                               text  \\\n",
       "0       [darrell, lucus]  [house, dem, aide, we, didn, even, see, comey,...   \n",
       "1        [daniel, flynn]  [ever, get, the, feeling, your, life, circles,...   \n",
       "2  [consortiumnews, com]  [why, the, truth, might, get, you, fired, octo...   \n",
       "3     [jessica, purkiss]  [videos, 15, civilians, killed, in, single, us...   \n",
       "4      [howard, portnoy]  [print, an, iranian, woman, has, been, sentenc...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      0  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_column(text):\n",
    "    # remove punctuation and special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    \n",
    "    # remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    \n",
    "    # replacing multi spaces with single ones\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "    # converting to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# apply preprocessing function to all string based columns\n",
    "train_data[['title', 'author', 'text']] = train_data[['title', 'author', 'text']].applymap(preprocess_column)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                              title  \\\n",
      "0   0  [house, dem, aide, we, didn, even, see, comey,...   \n",
      "1   1  [flynn, hillary, clinton, big, woman, on, camp...   \n",
      "2   2          [why, the, truth, might, get, you, fired]   \n",
      "3   3  [15, civilians, killed, in, single, us, airstr...   \n",
      "4   4  [iranian, woman, jailed, for, fictional, unpub...   \n",
      "\n",
      "                  author                                               text  \\\n",
      "0       [darrell, lucus]  [house, dem, aide, we, didn, even, see, comey,...   \n",
      "1        [daniel, flynn]  [ever, get, the, feeling, your, life, circle, ...   \n",
      "2  [consortiumnews, com]  [why, the, truth, might, get, you, fired, octo...   \n",
      "3     [jessica, purkiss]  [video, 15, civilian, killed, in, single, u, a...   \n",
      "4      [howard, portnoy]  [print, an, iranian, woman, ha, been, sentence...   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      0  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n"
     ]
    }
   ],
   "source": [
    "# downloads before first run\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords_and_lemmatize(tokens):\n",
    "    # remove stop words\n",
    "    #tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    \n",
    "    # lemmatize the words\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# apply the function to 'text' column\n",
    "train_data['text'] = train_data['text'].apply(remove_stopwords_and_lemmatize)\n",
    "\n",
    "print(train_data.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the CountVectorizer\n",
    "vectorizer_bow = CountVectorizer()\n",
    "\n",
    "# join list of words back to strings for the the BoW vectorizer\n",
    "train_data['text_bow'] = train_data['text'].apply(' '.join)\n",
    "\n",
    "# fit and transform the 'text' column\n",
    "X_bow = vectorizer_bow.fit_transform(train_data['text_bow'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the TfidfVectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "\n",
    "# We'll use the same 'text_bow' column\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(train_data['text_bow'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and validation sets using TF-IDF vectorized data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tfidf, train_data['label'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89      3148\n",
      "           1       0.94      0.82      0.88      3092\n",
      "\n",
      "    accuracy                           0.89      6240\n",
      "   macro avg       0.89      0.89      0.89      6240\n",
      "weighted avg       0.89      0.89      0.89      6240\n",
      "\n",
      "\n",
      "Alpha:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Filip\\Python\\lib\\site-packages\\sklearn\\naive_bayes.py:629: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Filip\\Python\\lib\\site-packages\\sklearn\\naive_bayes.py:635: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91      3148\n",
      "           1       0.97      0.83      0.89      3092\n",
      "\n",
      "    accuracy                           0.90      6240\n",
      "   macro avg       0.91      0.90      0.90      6240\n",
      "weighted avg       0.91      0.90      0.90      6240\n",
      "\n",
      "\n",
      "Alpha:  0.2\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90      3148\n",
      "           1       0.98      0.80      0.88      3092\n",
      "\n",
      "    accuracy                           0.89      6240\n",
      "   macro avg       0.91      0.89      0.89      6240\n",
      "weighted avg       0.90      0.89      0.89      6240\n",
      "\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89      3148\n",
      "           1       0.98      0.78      0.87      3092\n",
      "\n",
      "    accuracy                           0.88      6240\n",
      "   macro avg       0.90      0.88      0.88      6240\n",
      "weighted avg       0.90      0.88      0.88      6240\n",
      "\n",
      "\n",
      "Alpha:  0.4\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89      3148\n",
      "           1       0.98      0.76      0.86      3092\n",
      "\n",
      "    accuracy                           0.87      6240\n",
      "   macro avg       0.89      0.87      0.87      6240\n",
      "weighted avg       0.89      0.87      0.87      6240\n",
      "\n",
      "\n",
      "Alpha:  0.5\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88      3148\n",
      "           1       0.98      0.74      0.85      3092\n",
      "\n",
      "    accuracy                           0.87      6240\n",
      "   macro avg       0.89      0.87      0.86      6240\n",
      "weighted avg       0.89      0.87      0.86      6240\n",
      "\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88      3148\n",
      "           1       0.98      0.73      0.84      3092\n",
      "\n",
      "    accuracy                           0.86      6240\n",
      "   macro avg       0.88      0.86      0.86      6240\n",
      "weighted avg       0.88      0.86      0.86      6240\n",
      "\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.99      0.87      3148\n",
      "           1       0.99      0.71      0.82      3092\n",
      "\n",
      "    accuracy                           0.85      6240\n",
      "   macro avg       0.88      0.85      0.85      6240\n",
      "weighted avg       0.88      0.85      0.85      6240\n",
      "\n",
      "\n",
      "Alpha:  0.8\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.99      0.87      3148\n",
      "           1       0.99      0.70      0.82      3092\n",
      "\n",
      "    accuracy                           0.84      6240\n",
      "   macro avg       0.88      0.84      0.84      6240\n",
      "weighted avg       0.88      0.84      0.84      6240\n",
      "\n",
      "\n",
      "Alpha:  0.9\n",
      "Classification report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.99      0.86      3148\n",
      "           1       0.99      0.68      0.81      3092\n",
      "\n",
      "    accuracy                           0.84      6240\n",
      "   macro avg       0.87      0.84      0.83      6240\n",
      "weighted avg       0.87      0.84      0.83      6240\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trying different alpha values\n",
    "alphas = np.arange(0, 1, 0.1)\n",
    "\n",
    "# function for training with different alphas\n",
    "def train_and_predict(alpha):\n",
    "\n",
    "    # initialize Multinomial Naive Bayes classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "\n",
    "    # training\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "   # validation\n",
    "    y_pred_nb = nb_classifier.predict(X_val)\n",
    "\n",
    "    # compute classification report\n",
    "    clf_report = classification_report(y_val, y_pred_nb)\n",
    "\n",
    "    return clf_report\n",
    "\n",
    "# iterate over the alphas and print the corresponding report\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Classification report: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      3148\n",
      "           1       0.94      0.94      0.94      3092\n",
      "\n",
      "    accuracy                           0.94      6240\n",
      "   macro avg       0.94      0.94      0.94      6240\n",
      "weighted avg       0.94      0.94      0.94      6240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize Logistic Regression classifier\n",
    "clf_lr = LogisticRegression()\n",
    "\n",
    "# training\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "# validation\n",
    "y_pred_lr = clf_lr.predict(X_val)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_val, y_pred_lr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that the Logistic Regression model is performing better than the Naive Bayes regarding the general accuracy: It correctly identified both reliable and unreliable news articles better. Moreover, it's not just making safe predictions (which would give high precision but low recall), but it also does very well at identifying most of the actual instances of each class (high recall).\n",
    "\n",
    "However, the results of Logistic Regression model looks suspicious and should have been inspected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
